{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML5 Notebook\n",
    "\n",
    "In the video, you learnt about the K-Nearest Neighbour classifier (K-NN).\n",
    "\n",
    "<img src=\"./knn.png\" title=\"knn\"/>\n",
    "\n",
    "This has a **single hyperparameter K** and uses the training set to predict a classification label for a test point.\n",
    "\n",
    "We will work through this classifier in Python for the custom iris dataset we created in the last notebook.\n",
    "\n",
    "This contains 100 4D data points labelled as either **versicolor (0)** or **virginica (1)**. 40 of these points form the training set, 20 for validation, and 40 for test.\n",
    "\n",
    "Let's start by loading this data, and then follow the steps for K-NN to classify a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Import numpy\n",
    "from scipy import stats # for computing modes\n",
    "\n",
    "\n",
    "def load_iris():\n",
    "    \"\"\"\n",
    "    This function loads in all our dataset splits.\n",
    "    \"\"\"\n",
    "    D = np.load('iris_splits.npz')\n",
    "    return D['X_train'], D['y_train'], D['X_val'], D['y_val'], D['X_test'], D['y_test']\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first data point in our test matrix to use as our test point $\\mathbf{x}_t$. We'll then follow the K-NN algorithm, using the training data to classify it. We'll set $K=3$ (this could however, be optimised using the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = X_test[0] # Get the first (or 0th) data point in our test set\n",
    "print(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compute the L2 distance between $\\mathbf{x}_t$ and each training point $\\{\\mathbf{{x}}_{i}\\}_{i=1}^{N}$\n",
    "\n",
    "Given a training point $\\mathbf{x}_i\\in \\mathbb{R}^{D}$, the distance between it and a test point $\\mathbf{x}_t\\in \\mathbb{R}^{D}$ is  $|| \\mathbf{x}_i - \\mathbf{x}_t ||_2 = \\sqrt{\\sum_{j=1}^D (x_{i,j} - x_{t,j})^2}$\n",
    "\n",
    "Let's compute this for each training point, and store the values in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_from_xt = np.zeros(len(X_train)) # Create an array to populate with distances\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    distance_from_xt[i] = np.linalg.norm(X_train[i]-x_t) # Compute the distance from each training point\n",
    "\n",
    "print(np.round(distance_from_xt,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an array, where each entry is the distance between each training point and our test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Identify the K closest points to $\\mathbf{x}_t$ - those with the lowest L2 distances from it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `np.argsort` function for this purpose. It arranges an array from lowest to highest value, and then returns the **indices** for those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbours = np.argsort(distance_from_xt) # Sort distances and get the training point indices\n",
    "print(nearest_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first element in this new array is `28`. If we count (from 0) in the distance array, we can see that the 28th element of that is 1.88 which is the smallest distance.\n",
    "\n",
    "We have $K=3$ so we are interested in the first three values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_nearest_neighbours = nearest_neighbours[0:3] # Get the indices for the nearest 3 points\n",
    "print(k_nearest_neighbours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the K-Nearest Neighbours in the training set for this test point are those corresponding to `X_train[28]`, `X_train[4]` and `X_train[12]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assign $\\mathbf{x}_t$ to the most prevalent class of its nearest neighbours\n",
    "\n",
    "We now need to look at the labels for these nearby training data points. We can do this by consulting `y_train`, which is the array of corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_train[k_nearest_neighbours]\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all correspond to class 1 (virginica). This is therefore the most prevalent class. We can check the true label of our test point to see if the classifier got it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did get it right! See if you can repeat this process for another test point. It might be useful to use `stats.mode` to get the most prevelant class in `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The most prevelant class is {stats.mode(labels)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "I am now going to provide some Python code I have written that performs K-NN classification for the entire test set.  It then compares the classifier predictions to the actual test labels to compute an accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # import numpy for arrays\n",
    "import scipy.spatial # scipy spatial for computing distances in parallel\n",
    "from scipy import stats # stats for mode calculation\n",
    "\n",
    "\n",
    "def knn_classifier(X_train, y_train, eval_data, k):\n",
    "    \"\"\"\n",
    "    Performs K-NN classification on eval_data by computing distances of each of its points with\n",
    "    a training set X_train. The function outputs an array of predictions for each datapoint in eval_data\n",
    "    Args:\n",
    "        X_train : training set represented by a matrix (NxD) of N data points of dimensionality D\n",
    "        y_train : labels for the training set, a vector N where each element i corresponds to the label of X_train[i]\n",
    "        eval_data : a dataset of test (or validation) points \n",
    "    Example use:\n",
    "        >>> val_predictions = knn_classifier(X_train, y_train, X_val, k=3)\n",
    "        >>> test_predictions = knn_classifier(X_train, y_train, X_test, k=5)\n",
    "    \"\"\"\n",
    "\n",
    "    dist_mat = scipy.spatial.distance_matrix(eval_data,X_train) # Get the distance matrix between all points\n",
    "    sorted_neighbours = np.argsort(dist_mat) # Sort by distance \n",
    "    knns = sorted_neighbours[:,0:k] # Take k nearest neighbours\n",
    "    neighbour_labels = y_train[knns] # Get the labels for every neighbour for every point\n",
    "    predictions, _  = stats.mode(neighbour_labels,axis=1) # First output of stats.mode is the actual mode\n",
    "    return np.squeeze(predictions) # Squeeze gets rid of unnecessary extra dimensions\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, true_labels):\n",
    "    \"\"\"\n",
    "    Compares predicted labels in predictions to the actual labels in true_labels and computes an accuracy score.\n",
    "    Args:\n",
    "        predictions : an N-D array of predictions obtained from a classifier for some data points\n",
    "        true_labels : an N-D array of ground-truth labels \n",
    "    Example use:\n",
    "        >>> val_accuracy = compute_accuracy(val_predictions, y_val)\n",
    "        >>> test_accuracy = compute_accuracy(test_predictions, y_test)\n",
    "    \n",
    "    \"\"\"\n",
    "    return 100*np.sum(predictions==true_labels)/len(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_classifier(X_train, y_train, X_test, k=3) # Gets classifier predictions for X_test\n",
    "print(predictions) # Let's see what classes have been predicted\n",
    "acc = compute_accuracy(predictions, y_test) # Compares these predictions to the true labels to get an accuracy score\n",
    "print(f'Our K-NN classifier with K=3 is {acc:0.2f}% accurate.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
