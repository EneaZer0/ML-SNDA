{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML3 Notebook\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "We have a dataset matrix $\\mathbf{X}\\in \\mathbb{R}^{N\\times D}$ that consists of $N$ data points, each of dimensionality $D$. We want to reduce the dimensionality of our dataset, so it takes up less space, and is easier to visualise. \n",
    "\n",
    "We can achieve this by using a transformation matrix $\\mathbf{W}\\in \\mathbb{R}^{D\\times M}$. This transforms our data to a lower dimension using $ \\mathbf{X_{D}} = \\mathbf{X}\\mathbf{W}$.\n",
    "\n",
    "To determine how good this transformation is, we can attempt to reconstruct our data using $ \\mathbf{\\widetilde{X}} =  \\mathbf{X_D}\\mathbf{W^T}$. We can then compute reconstruction error.\n",
    "\n",
    "The transformation that **minimises reconstruction error** in this scenario is the matrix that consists of the first $M$ **principal components** of $\\mathbf{X}$. We will compute these for the iris dataset in this notebook by following the PCA algorithm:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Import numpy\n",
    "X = np.load('iris_standardised.npy') # Load our standardised iris data\n",
    "\n",
    "print(f'Our dataset is represented by a matrix X that is of shape {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make sure your data is standarised.\n",
    "\n",
    "This dataset has already been standardised!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Construct the covariance matrix $\\mathbf{C} =\\frac{1}{D}\\mathbf{X^T}\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 4 \n",
    "C = (1/D)*X.T@X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute the eigenvalues/vectors of $\\mathbf{C}$ ensuring that they are unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done numerically using `np.linalg.eig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "\n",
    "print(f'The eigenvalues of C are {eigenvalues}')\n",
    "print(f'The eigenvectors of C are \\n {eigenvectors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the eigenvalues are arranged highest to lowest. Each column in the matrix above contains the  an eigenvector corresponding to each eigenvalue. These are already unit norm (thanks Python!). This means that the first (or 0th in Python) column of `eigenvectors` is the 1st principal component, the second is the second principal component etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the $\\mathbf{W}$ that minimises reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to reduce our dataset from 4D to 2D, then the $\\mathbf{W}$ that minimises reconstruction error (and is therefore a good transformation) is given by the matrix containing the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = eigenvectors[:,0:2] # Python notation takes some getting used to, but this is grabbing the first two columns of W\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this to compute reconstruction error, and confirm that it is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_D = X@W # Use W to reduce X down to XD\n",
    "X_tilde = X@W@W.T # Use W to reduce X down, and then W.T to bring it back up to attempt reconstruction\n",
    "E = np.sum(np.sum((X-X_tilde)**2,1)**.5) # Compute reconstruction error\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is several times lower than our naive choice of $\\mathbf{W}$ but is non-zero. That's because **in most cases we can't have perfect reconstruction if we throw away information**. It's a compromise. \n",
    "\n",
    "Let's see what happens if we instead go from 4D to 3D, using the $\\mathbf{W}$ that minimises reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = eigenvectors[:,0:3] # Get the first three principal components\n",
    "print(W)\n",
    "X_D_3D = X@W # Use W to reduce X down to XD\n",
    "X_tilde = X@W@W.T # Use W to reduce X down, and then W.T to bring it back up to attempt reconstruction\n",
    "E = np.sum(np.sum((X-X_tilde)**2,1)**.5) # Compute reconstruction error\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a lower error, because we are throwing away less information by going down to 3D. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising our data\n",
    "\n",
    "So what does our data look like? We can't plot data in 4 dimensions. Let's just take a look at the first two. We will also colour each point to correspond to the type of flower it has been labelled as. **Each flower in the dataset has been labelled as either setosa, virginica and versicolor.**\n",
    "\n",
    "Let's plot the first two dimensions of our **original standardised data** (sepal length, sepal width). Note that these are not the same as the two new dimensions that PCA gives us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
    "import sklearn.datasets # If you are running this locally, then `pip install sklearn` in your Python environment.\n",
    "iris = sklearn.datasets.load_iris() #Get the flower labels from sklearn.\n",
    "\n",
    "\n",
    "species0 = iris.target == 0 # Find flowers labelled as class 0 \n",
    "species1 = iris.target == 1 # Find flowers labelled as class 1 \n",
    "species2 = iris.target == 2 # Find flowers labelled as class 2 \n",
    "\n",
    "plt.scatter(X[species0,0],X[species0,1] ,color='b') # Plot standardised data in class 0 in blue\n",
    "plt.scatter(X[species1,0],X[species1,1], color='g') # Plot standardised data in class 1 in green\n",
    "plt.scatter(X[species2,0],X[species2,1], color='r') # Plot standardised data in class 2 in red\n",
    "\n",
    "plt.xlabel('sepal length (standardised)') # Label your axes!\n",
    "plt.ylabel('sepal width (standardised)') # Label your axes!\n",
    "    \n",
    "plt.legend(iris.target_names)  # Put class names in the legend\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite difficult in this space to be able to tell veriscolor, and virginica apart. This will be difficult for classification (which we will learn about soon). Let's see what our space looks like once we've used $\\mathbf{W}$ to take the original data down to 2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_D[species0,0],X_D[species0,1] ,color='b') # Plot PCA projected data in class 0 in blue\n",
    "plt.scatter(X_D[species1,0],X_D[species1,1], color='g') # Plot PCA projected data in class 1 in green\n",
    "plt.scatter(X_D[species2,0],X_D[species2,1], color='r') # Plot PCA projected data in class 2 in red\n",
    "\n",
    "plt.xlabel('PCA dimension 1') # Label your axes!\n",
    "plt.ylabel('PCA dimension 2') # Label your axes!\n",
    "    \n",
    "plt.legend(iris.target_names)  # Put class names in the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three types of flowers are now more distinct! By using a transformation to minimise reconstruction error, the data lies in a space where these is maximal **variation**. In other words, what PCA has done is found the best linear combination of the original 4 dimensions such that data is varied as much as possible in 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish by viewing our data reduced to 3D using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_D_3D[species0,0],X_D_3D[species0,1],X_D_3D[species0,2] ,color='b') # Plot PCA projected data in class 0 in blue\n",
    "ax.scatter(X_D_3D[species1,0],X_D_3D[species1,1],X_D_3D[species0,2], color='g') # Plot PCA projected data in class 1 in green\n",
    "ax.scatter(X_D_3D[species2,0],X_D_3D[species2,1],X_D_3D[species0,2], color='r') # Plot PCA projected data in class 2 in red\n",
    "\n",
    "ax.set_xlabel('PCA dimension 1') # Label your axes!\n",
    "ax.set_ylabel('PCA dimension 2') # Label your axes!\n",
    "ax.set_zlabel('PCA dimension 3') # Label your axes!\n",
    "\n",
    "plt.legend(iris.target_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
