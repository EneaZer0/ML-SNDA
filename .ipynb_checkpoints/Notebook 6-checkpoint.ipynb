{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML6 Notebook\n",
    "\n",
    "In the video, you learnt what a linear classifier is, and how its parameters can be learnt.\n",
    "\n",
    "We will work through creating and training a linear classifier in Python for the custom iris set we created. **This is the longest, and probably the most important notebook!**\n",
    "\n",
    "Recall that our custom iris dataset consists of 100 4D data points labelled as either **versicolor (0)** or **virginica (1)**. 40 of these points form the training set, 20 form validation, and 40 form test.\n",
    "\n",
    "Formally, our training set is $\\{\\mathbf{{x}}_{i}\\}_{i=1}^{N}$ ($N=40$) where each $\\mathbf{x}_i\\in \\mathbb{R}^{4}$. Each datum has an associated label $y_i\\in \\{0, 1\\}$ where $0$ corresponds to versicolor, and $1$ corresponds to virginica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Import numpy\n",
    "\n",
    "\n",
    "def load_iris():\n",
    "    \"\"\"\n",
    "    This function loads in all our dataset splits.\n",
    "    \"\"\"\n",
    "    D = np.load('iris_splits.npz')\n",
    "    return D['X_train'], D['y_train'], D['X_val'], D['y_val'], D['X_test'], D['y_test']\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a linear classifier\n",
    "\n",
    "Let's start by creating a class to represent our classifier. As we are in 4D, our classifier will have a weight vector $\\mathbf{w}\\in \\mathbb{R}^{4}$, and a bias $b\\in \\mathbb{R}^{1}$. We will initialise these at random. We will not absorb the bias for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5) # Fix the seed so we have the same random numbers each time we run this.\n",
    "\n",
    "class LinearClassifier:\n",
    "    \"\"\"\n",
    "    This class creates a linear classifer object with randomly initialised weight and bias.\n",
    "    Its apply function can be used to apply the classifier to a data point.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_dimensions=4):\n",
    "        self.w = np.random.normal(size=num_dimensions) # Create a weight vector \n",
    "        self.b = np.random.normal(size=1) # Create a bias\n",
    "        \n",
    "    def apply(self, x):\n",
    "        \"\"\"\n",
    "        This function takes in a data point x, and applies the classifier to it.\n",
    "        \"\"\"\n",
    "        return x@self.w.T + self.b # Computes the classifier output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a classifier object from this class, and apply it to a dummy data point. Remember that a linear classifier takes a data point $\\mathbf{x}_i$ and outputs\n",
    "\n",
    "$$z_i = \\mathbf{x}_i\\mathbf{w}^\\top + b$$ \n",
    "\n",
    "where $z_i\\in \\mathbb{R}^{1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.17304207]\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearClassifier() # Make classifier\n",
    "x_i = np.array([1,2,3,4]) # Create a dummy data point. Remember we are working in 4D\n",
    "z_i = classifier.apply(x_i) # Apply classifier to data point.\n",
    "print(z_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "A classifier isn't very useful with random parameters, so we are going to learn these by training the classifier with a training set.\n",
    "\n",
    "To do this we need \n",
    "\n",
    "- to devise a loss function, that is **small** when our classifier does what we want it to do \n",
    "- an optimisation algorithm to minimise that loss function\n",
    "\n",
    "Following the video, we are going to use the **Mean Squared Error (MSE) loss** where want a classifier to output $z_i=1$ if $y_i=1$ and $z_i=0$ if $y_i=0$ for each training point.\n",
    "\n",
    "$$L = \\frac{1}{N}\\sum_{i=1}^N (z_i - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2$$\n",
    "\n",
    "Let's compute the MSE loss for our randomly initalised classifier.\n",
    "\n",
    "First, let's get the classifier outputs for our training data. The way we have written the `apply` function means we can apply the classifier to the whole training set in one go, in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71291564  0.55332369  0.72863524  1.23264337  2.75705781  1.75422019\n",
      "  0.16450055  1.26631083  0.88625247  0.81937071  2.23414059  1.77030467\n",
      "  2.87350859  1.14678558  1.93609405  0.83444717  0.77527009  4.31085347\n",
      "  1.25110918  2.47026795  2.62596991  0.88466739  2.25496317  0.65534408\n",
      "  1.2451126   1.6125761   1.91926577  1.4921718   4.7302972   2.57945594\n",
      "  1.34600529  1.4714584   1.73184239  2.18525816 -0.87644074  2.82299954\n",
      "  5.09227622  2.99253792  0.74913974  1.36808592]\n"
     ]
    }
   ],
   "source": [
    "outputs = classifier.apply(X_train) # Apply the classifier to the training set\n",
    "print(outputs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a function that computes MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.632312885355284\n"
     ]
    }
   ],
   "source": [
    "def get_loss(outputs, labels):\n",
    "    \"\"\"\n",
    "    This function takes classifier outputs for data points, and the true labels to compute MSE.\n",
    "    \"\"\"\n",
    "    return np.mean((outputs-labels)**2)\n",
    "\n",
    "loss = get_loss(outputs, y_train)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classifier to do what we want it to do, we want this MSE to be as low as possible.\n",
    "\n",
    "We will use an **optimisation algorithm** to find the **classifier parameters that minimise this loss function**.\n",
    "\n",
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a loss function $L$ that is a function of some parameter vector $\\mathbf{w}$, we can minimise $L$ using the gradient descent algorithm.\n",
    "\n",
    "<img src=\"./grad.png\" title=\"grad\"/>\n",
    "\n",
    "Let our initial parameter vector be $\\mathbf{w}_{t=0}$. The gradient of the loss with respect to this vector $\\frac{\\partial L}{\\partial \\mathbf{w}_{t=0}}$ is, locally, the vector that points in the direction that we can move $\\mathbf{w}_{t=0}$ in to most **increase** the loss.\n",
    "\n",
    "We want to decrease the loss, so we need to move $\\mathbf{w}_{t=0}$ in the **opposite direction**.\n",
    "\n",
    "To do this, we subtract this vector (multiplied by a constant $\\alpha$) from $\\mathbf{w}_{t=0}$:\n",
    "\n",
    "$$\\mathbf{w}_{t=1} = \\mathbf{w}_{t=0} - \\alpha \\frac{\\partial L}{\\partial \\mathbf{w}_{t=0}}$$\n",
    "\n",
    "$\\alpha$ is a hyper-parameter called the learning rate, which is typically set to a small value.\n",
    "\n",
    "We then repeat this process using $\\frac{\\partial L}{\\partial \\mathbf{w}_{t=1}}$ to compute $\\mathbf{w}_{t=2}$, and so on until we have made our loss as small as possible.\n",
    "\n",
    "What do we do about $b$? We initialise it as $b_{t=0}$ and update it alongside $\\mathbf{w}$ by computing $\\frac{\\partial L}{\\partial b_t}$ at each step, and adjusting it in a similar way.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are these gradients?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write expressions for these gradients ourselves analytically, and then plug these into Python.\n",
    "\n",
    "Recall that we have \n",
    "$$L = \\frac{1}{N}\\sum_{i=1}^N (z_i - y_i)^2 = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2$$\n",
    "\n",
    "Computing $\\frac{\\partial L}{\\partial b}$ is quite straightforward if you remember that differentiation and summation are both **linear** which means we can swap their order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L  = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b}  = \\frac{\\partial}{\\partial b}\\big(\\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2\\big)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b}  = \\frac{1}{N}\\sum_{i=1}^N \\frac{\\partial}{\\partial b}\\big((\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2\\big)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b}  = \\frac{2}{N}\\sum_{i=1}^N(\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i) = \\frac{2}{N}\\sum_{i=1}^N(z_i - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing $\\frac{\\partial L}{\\partial \\mathbf{w}}$ is more challenging. In the slides, I wrote this vector out in terms of its elements\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial {\\mathbf{w}}} =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial L}{\\partial {w_1}}&\\frac{\\partial L}{\\partial {w_2}}&... \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "While it is possible to individually compute each of the derivative elements separately, I will instead turn to a result from vector calculus.\n",
    "\n",
    "$$\\frac{d}{d\\mathbf{w}}(\\mathbf{x}\\mathbf{w}^\\top)= \\mathbf{x}$$\n",
    "\n",
    "Armed with this, we can compute $\\frac{\\partial L}{\\partial \\mathbf{w}}$:\n",
    "\n",
    "$$L  = \\frac{1}{N}\\sum_{i=1}^N (\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}}  = \\frac{1}{N}\\sum_{i=1}^N \\frac{\\partial}{\\partial \\mathbf{w}}\\big((\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i)^2\\big)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}}   = \\frac{2}{N}\\sum_{i=1}^N\\mathbf{x}_i (\\mathbf{x}_i \\mathbf{w}^\\top + b - y_i) = \\frac{2}{N}\\sum_{i=1}^N \\mathbf{x}_i  (z_i - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Gradient Descent\n",
    "\n",
    "Now that we have expressions for the gradients, we are going to write some code to perform gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier_using_GD(classifier, X_train, y_train, alpha = 0.1, num_iters = 100):\n",
    "    \"\"\"\n",
    "    This function takes in a classifier object, training data, labels, and a learning\n",
    "    rate alpha and performs iterations of gradient descent to update the classifier's\n",
    "    parameters. It uses the gradients of the MSE loss. The function will change the\n",
    "    parameter weights of the classifier object, and outputs the MSE loss for each iteration.\n",
    "    Args:\n",
    "        classifier : A classifier object as defined above in this notebook.\n",
    "        X_train : training set represented by a matrix (NxD) of N data points of dimensionality D\n",
    "        y_train : labels for the training set, a vector N where each element i corresponds to the label of X_train[i]\n",
    "        alpha : The learning rate, default 0.1\n",
    "        num_iters : The number of iterations to perform gradient descent, default 100\n",
    "    Example use:\n",
    "        >>> loss_array = train_classifier_using_GD(classifier, X_train, y_train)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_data_points, num_dimensions = X_train.shape # Get num data points, and dims\n",
    "    loss_array = [] # Empty array to append the MSE loss at each iteration.\n",
    "    \n",
    "    for _ in range(num_iters):\n",
    "        \n",
    "        z = classifier.apply(X_train) # Apply the classifier to the training points\n",
    "         \n",
    "        dL_dw =  np.zeros(num_dimensions) # Create a zero array to populate\n",
    "        dL_db = 0 # Zero value for bias\n",
    "        \n",
    "        for i in range(num_data_points):\n",
    "            dL_dw += X_train[i]*(z[i]-y_train[i]) # Sum for each data point\n",
    "            dL_db += (z[i]-y_train[i]) # Sum for each data point\n",
    "\n",
    "        dL_dw *= 2/num_data_points # Multiply by 2/N\n",
    "        dL_db *= 2/num_data_points # Multiply by 2/N\n",
    "        \n",
    "        # Now, update the parameters\n",
    "        classifier.w -= alpha * dL_dw \n",
    "        classifier.b -= alpha * dL_db\n",
    "        \n",
    "        # (Re)compute loss and append to loss array\n",
    "        z = classifier.apply(X_train) \n",
    "        loss = np.mean((z-y_train)**2)\n",
    "        loss_array.append(loss)\n",
    "\n",
    "    return loss_array\n",
    "        \n",
    "            \n",
    "loss_array = train_classifier_using_GD(classifier, X_train, y_train) # Perform gradient descent to learn our classifier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now performed 100 iterations of gradient descent to train our classifier (i.e. adjust its parameters so that MSE loss is minimised).\n",
    "\n",
    "The code above records the loss after each iteration. Let's see how it changed as we iterated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt # Plotting code\n",
    "plt.plot(loss_array)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that performing gradient descent has continuously updated the parameters of our classifier to make the loss small! Now that we have learnt our classifier, let's apply it to the test set and see how accurate it is.\n",
    "\n",
    "As per the slides, we will count the classifier prediction as being for class 0 (veriscolor) if its output is less than 0.5, and being for class 1 (virginica) if it is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our trained linear classifier is 92.50% accurate.\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(outputs, true_labels):\n",
    "    \"\"\"\n",
    "    Compares predicted labels in predictions to the actual labels in true_labels and computes an accuracy score.\n",
    "    Args:\n",
    "        predictions : an N-D array of predictions obtained from a classifier for some data points\n",
    "        true_labels : an N-D array of ground-truth labels \n",
    "    Example use:\n",
    "        >>> val_accuracy = compute_accuracy(val_predictions, y_val)\n",
    "        >>> test_accuracy = compute_accuracy(test_predictions, y_test)\n",
    "    \"\"\"\n",
    "    predictions = np.zeros_like(outputs)\n",
    "    for i in range(len(outputs)):\n",
    "        if outputs[i] > 0.5: # Classify as 1 if >0.5 and as 0 if <0.5\n",
    "            predictions[i] = 1\n",
    "        else:\n",
    "            predictions[i] = 0\n",
    "\n",
    "    return 100*np.sum(predictions==true_labels)/len(predictions)\n",
    "\n",
    "\n",
    "outputs = classifier.apply(X_test) # Apply learnt classifier to test points to get predictions\n",
    "acc = compute_accuracy(outputs, y_test) # See if the predictions were accurate.\n",
    "print(f'Our trained linear classifier is {acc:0.2f}% accurate.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
